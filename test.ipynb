{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#import matplotlib.pyplot as plt\n",
    "# Note! ITK interacts weirdly here.  from lazy_imports import itk does not work.\n",
    "# Additionally, import itk must occur before lazy_imports for itkwidgets.view (ie itkview) to work.\n",
    "import itk\n",
    "#from lazy_imports import itk\n",
    "#from lazy_imports import torch\n",
    "import torch # to avoid sym3eig_cpu ImportError: libc10.so: cannot open shared object file: No such file or directory\n",
    "from lazy_imports import np\n",
    "from lazy_imports import plt\n",
    "from lazy_imports import sitk\n",
    "from lazy_imports import loadmat, savemat\n",
    "from lazy_imports import sitk\n",
    "from lazy_imports import itkwidgets\n",
    "from lazy_imports import itkview\n",
    "from lazy_imports import interactive\n",
    "from lazy_imports import ipywidgets\n",
    "from lazy_imports import pv\n",
    "#from lazy_imports import vlinalg\n",
    "from torch_sym3eig import Sym3Eig\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6) # (w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtch.SplitEbinMetric3D import get_karcher_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.diffeo import get_idty_3d, get_gradient_3d, compose_function_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtch.metricMatching import metric_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disp.vis import show_2d, show_2d_tensors\n",
    "from disp.vis import vis_tensors, vis_path\n",
    "from disp.vis import view_3d_tensors, tensors_to_mesh, view_3d_paths, path_to_tube\n",
    "from data.io import readRaw, ReadScalars, ReadTensors, WriteTensorNPArray, WriteScalarNPArray, readPath3D\n",
    "from data.convert import GetNPArrayFromSITK, GetSITKImageFromNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_GPU=False\n",
    "#do_GPU=True\n",
    "\n",
    "use_brain=True\n",
    "#use_brain=False\n",
    "\n",
    "if do_GPU:\n",
    "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "  torch.cuda.set_device(1)\n",
    "else:\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown in SimpleITK ReadImage: /tmp/SimpleITK/Code/IO/src/sitkImageReaderBase.cxx:99:\nsitk::ERROR: The file \"/Volumes/GoogleDrive/Shared\\ drives/NSFCRCNSHumanConectome/Data/prepped_data/103818/dti_1000_tensor.nhdr\" does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5772280665a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtens_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'dti_{bval}_tensor.nhdr'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0min_tens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReadTensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtens_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mtens_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'scaled_tensors.nhdr'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mout_tens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReadTensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtens_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/Atlas-Building-3D_clean/data/io.py\u001b[0m in \u001b[0;36mReadTensors\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mReadTensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetNPArrayFromSITK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReadImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mReadScalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/py3_venv_nsfcrcns/lib64/python3.6/site-packages/SimpleITK/SimpleITK.py\u001b[0m in \u001b[0;36mReadImage\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   8874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8875\u001b[0m     \"\"\"\n\u001b[0;32m-> 8876\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_SimpleITK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReadImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8877\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8878\u001b[0m     \"\"\"\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception thrown in SimpleITK ReadImage: /tmp/SimpleITK/Code/IO/src/sitkImageReaderBase.cxx:99:\nsitk::ERROR: The file \"/Volumes/GoogleDrive/Shared\\ drives/NSFCRCNSHumanConectome/Data/prepped_data/103818/dti_1000_tensor.nhdr\" does not exist."
     ]
    }
   ],
   "source": [
    "if use_brain:\n",
    "  cases = []\n",
    "  #all_start_coords = []\n",
    "  cases.append('103818')\n",
    "  #all_start_coords.append([[62,126,56]])\n",
    "  cases.append('105923')\n",
    "  #all_start_coords.append([[61,125,56]])\n",
    "  #cases.append('111312')\n",
    "  #all_start_coords.append([[62,128,56]])\n",
    "  masks = []\n",
    "  t1s = []\n",
    "  in_tensors = []\n",
    "  out_tensors = []\n",
    "  outdirs = []\n",
    "  for run_case in cases:\n",
    "    subj = run_case[:6]\n",
    "    #outroot='/usr/sci/projects/HCP/Kris/NSFCRCNS/TestResults/working_3d_python/simulation_results/'\n",
    "    #outdir = f'{outroot}{run_case}/mineval_0.005_n_3000_s_1.5/'\n",
    "    #indir = '/usr/sci/projects/HCP/Kris/NSFCRCNS/prepped_data/' + subj + '/'\n",
    "\n",
    "    outroot='/Volumes/GoogleDrive/Shared\\ drives/NSFCRCNSHumanConectome/Data/3D_results/'\n",
    "    outdir = f'{outroot}{run_case}/'\n",
    "    indir = '/Volumes/GoogleDrive/Shared\\ drives/NSFCRCNSHumanConectome/Data/prepped_data/' + subj + '/'\n",
    "\n",
    "    #bval = 3000\n",
    "    #outroot='/usr/sci/projects/HCP/Kris/NSFCRCNS/TestResults/working_3d_python/UKF_experiments/'\n",
    "    #indir = '/usr/sci/projects/HCP/Kris/NSFCRCNS/prepped_UKF_data_with_grad-dev/' + subj + '/'\n",
    "    bval = 1000\n",
    "    \n",
    "    outdirs.append(outdir)\n",
    "\n",
    "    tens_file = f'dti_{bval}_tensor.nhdr'\n",
    "    in_tens = ReadTensors(indir + tens_file)\n",
    "    tens_file = 'scaled_tensors.nhdr'\n",
    "    out_tens = ReadTensors(outdir + tens_file)\n",
    "    #t1_file = 't1_stripped_irescaled.nhdr'\n",
    "    #t1 = ReadScalars(indir + t1_file)\n",
    "    mask_file = 'filt_mask.nhdr'\n",
    "    mask = ReadScalars(outdir + mask_file)\n",
    "    in_tensors.append(in_tens)\n",
    "    out_tensors.append(out_tens)\n",
    "    #t1s.append(t1)\n",
    "    masks.append(mask)\n",
    "    #t1_flip = ReadScalars(indir + 't1_rescaled_flipped.nhdr')[:,::-1,:]\n",
    "\n",
    "\n",
    "  #atlasdir = '/home/sci/hdai/Shared/BrainAtlasMay21/'\n",
    "  #atlas_tens = ReadTensors(atlasdir + 'atlas_tens.nhdr')\n",
    "  #atlas_mask = ReadScalars(atlasdir + 'atlas_mask.nhdr')\n",
    "  atlasdir = '/home/sci/hdai/Shared/Brain3AtlasMay24/'\n",
    "  atlas_tens = ReadTensors(atlasdir + 'atlas_800_tens.nhdr')\n",
    "  atlas_mask = ReadScalars(atlasdir + 'atlas_800_mask.nhdr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 s, sys: 236 ms, total: 13.2 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if use_brain:\n",
    "  tens_0 = np.zeros((out_tensors[0].shape[0],out_tensors[0].shape[1],out_tensors[0].shape[2],3,3))\n",
    "  tens_1 = np.zeros((out_tensors[1].shape[0],out_tensors[1].shape[1],out_tensors[1].shape[2],3,3))\n",
    "  for xx in range(tens_0.shape[0]):\n",
    "    for yy in range(tens_0.shape[1]):\n",
    "      for zz in range(tens_0.shape[2]):\n",
    "        if masks[0][xx,yy,zz]:\n",
    "          tens_0[xx,yy,zz,0,0] = out_tensors[0][xx,yy,zz,0]\n",
    "          tens_0[xx,yy,zz,0,1] = out_tensors[0][xx,yy,zz,1]\n",
    "          tens_0[xx,yy,zz,1,0] = out_tensors[0][xx,yy,zz,1]\n",
    "          tens_0[xx,yy,zz,0,2] = out_tensors[0][xx,yy,zz,2]\n",
    "          tens_0[xx,yy,zz,2,0] = out_tensors[0][xx,yy,zz,2]\n",
    "          tens_0[xx,yy,zz,1,1] = out_tensors[0][xx,yy,zz,3]\n",
    "          tens_0[xx,yy,zz,1,2] = out_tensors[0][xx,yy,zz,4]\n",
    "          tens_0[xx,yy,zz,2,1] = out_tensors[0][xx,yy,zz,4]\n",
    "          tens_0[xx,yy,zz,2,2] = out_tensors[0][xx,yy,zz,5]\n",
    "        else:\n",
    "          tens_0[xx,yy,zz,0,0] = 1\n",
    "          tens_0[xx,yy,zz,1,1] = 1\n",
    "          tens_0[xx,yy,zz,2,2] = 1\n",
    "        if masks[1][xx,yy,zz]:\n",
    "          tens_1[xx,yy,zz,0,0] = out_tensors[1][xx,yy,zz,0]\n",
    "          tens_1[xx,yy,zz,0,1] = out_tensors[1][xx,yy,zz,1]\n",
    "          tens_1[xx,yy,zz,1,0] = out_tensors[1][xx,yy,zz,1]\n",
    "          tens_1[xx,yy,zz,0,2] = out_tensors[1][xx,yy,zz,2]\n",
    "          tens_1[xx,yy,zz,2,0] = out_tensors[1][xx,yy,zz,2]\n",
    "          tens_1[xx,yy,zz,1,1] = out_tensors[1][xx,yy,zz,3]\n",
    "          tens_1[xx,yy,zz,1,2] = out_tensors[1][xx,yy,zz,4]\n",
    "          tens_1[xx,yy,zz,2,1] = out_tensors[1][xx,yy,zz,4]\n",
    "          tens_1[xx,yy,zz,2,2] = out_tensors[1][xx,yy,zz,5]\n",
    "        else:\n",
    "          tens_1[xx,yy,zz,0,0] = 1\n",
    "          tens_1[xx,yy,zz,1,1] = 1\n",
    "          tens_1[xx,yy,zz,2,2] = 1\n",
    "        \n",
    "\n",
    "  if do_GPU:\n",
    "    torch_tens_0 = torch.from_numpy(tens_0).cuda().float()\n",
    "    torch_tens_1 = torch.from_numpy(tens_1).cuda().float()\n",
    "    mask_union = torch.from_numpy(masks[0]).cuda().float()\n",
    "    mask_union += torch.from_numpy(masks[1]).cuda().float()\n",
    "\n",
    "  else:\n",
    "    torch_tens_0 = torch.from_numpy(tens_0).float()\n",
    "    torch_tens_1 = torch.from_numpy(tens_1).float()\n",
    "    mask_union = torch.from_numpy(masks[0]).float()\n",
    "    mask_union += torch.from_numpy(masks[1]).float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 562705.8125\n",
      "1 537092.375\n",
      "2 557556.75\n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                               Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "               aten::_symeig_helper        60.06%       49.419s        60.28%       49.597s        8.266s       49.419s        59.99%       49.598s        8.266s             6  \n",
      "              aten::_inverse_helper         8.96%        7.369s         9.56%        7.865s     393.263ms        7.369s         8.95%        7.867s     393.328ms            20  \n",
      "             aten::triangular_solve         6.34%        5.214s         6.48%        5.333s        1.778s        5.214s         6.33%        5.334s        1.778s             3  \n",
      "                aten::_lu_with_info         4.48%        3.684s         5.07%        4.173s     231.842ms        3.685s         4.47%        4.175s     231.942ms            18  \n",
      "                          aten::mul         3.04%        2.504s         3.05%        2.510s       3.525ms        2.532s         3.07%        2.536s       3.562ms           712  \n",
      "                          aten::bmm         3.00%        2.470s         3.01%        2.473s      28.760ms        2.489s         3.02%        2.489s      28.946ms            86  \n",
      "                        aten::copy_         2.80%        2.306s         2.80%        2.306s       1.533ms        2.334s         2.83%        2.334s       1.552ms          1504  \n",
      "                        aten::index         2.34%        1.924s         2.35%        1.935s      15.118ms        1.961s         2.38%        1.965s      15.351ms           128  \n",
      "                         aten::_cat         1.70%        1.396s         1.77%        1.454s       1.871ms        1.453s         1.76%        1.476s       1.899ms           777  \n",
      "                          aten::add         1.59%        1.305s         1.59%        1.305s       3.145ms        1.335s         1.62%        1.335s       3.217ms           415  \n",
      "                     aten::cholesky         1.50%        1.232s         1.82%        1.499s     249.909ms        1.231s         1.49%        1.501s     250.124ms             6  \n",
      "                         aten::add_         0.73%     599.458ms         0.73%     599.458ms       4.541ms     613.803ms         0.75%     613.803ms       4.650ms           132  \n",
      "                          aten::sub         0.52%     423.951ms         0.59%     487.617ms       1.374ms     431.770ms         0.52%     495.719ms       1.396ms           355  \n",
      "                        aten::fill_         0.33%     268.816ms         0.33%     268.816ms     707.410us     278.116ms         0.34%     278.116ms     731.884us           380  \n",
      "                          aten::sum         0.31%     254.680ms         0.47%     387.903ms       8.081ms     261.837ms         0.32%     390.590ms       8.137ms            48  \n",
      "                    aten::remainder         0.30%     245.524ms         0.30%     245.524ms       5.115ms     250.200ms         0.30%     250.200ms       5.213ms            48  \n",
      "                          aten::div         0.19%     156.613ms         0.20%     163.428ms       1.112ms     166.678ms         0.20%     172.538ms       1.174ms           147  \n",
      "                          aten::pow         0.17%     135.840ms         0.18%     151.316ms       3.152ms     137.880ms         0.17%     154.242ms       3.213ms            48  \n",
      "                        aten::tril_         0.16%     131.809ms         0.16%     131.809ms      14.645ms     132.737ms         0.16%     132.737ms      14.749ms             9  \n",
      "                          aten::neg         0.13%     103.936ms         0.13%     103.936ms       1.283ms     103.910ms         0.13%     103.910ms       1.283ms            81  \n",
      "                      aten::nonzero         0.10%      86.099ms         0.11%      86.555ms       2.885ms      88.597ms         0.11%      88.694ms       2.956ms            30  \n",
      "                   aten::linalg_det         0.10%      80.932ms         5.83%        4.799s     266.617ms      80.490ms         0.10%        4.800s     266.643ms            18  \n",
      "                        aten::fmod_         0.10%      80.688ms         0.10%      80.688ms       4.483ms      80.258ms         0.10%      80.258ms       4.459ms            18  \n",
      "                         aten::sqrt         0.09%      77.890ms         0.09%      77.890ms      12.982ms      77.544ms         0.09%      77.544ms      12.924ms             6  \n",
      "                           aten::ne         0.08%      67.980ms         0.16%     135.327ms       3.759ms      71.253ms         0.09%     142.405ms       3.956ms            36  \n",
      "                         aten::prod         0.08%      62.855ms         0.09%      74.952ms       4.164ms      65.324ms         0.08%      76.380ms       4.243ms            18  \n",
      "                        aten::floor         0.07%      56.574ms         0.07%      56.574ms       1.179ms      57.946ms         0.07%      57.946ms       1.207ms            48  \n",
      "                   CholeskyBackward         0.05%      41.186ms         7.07%        5.817s        1.939s      41.158ms         0.05%        5.817s        1.939s             3  \n",
      "                         aten::roll         0.04%      34.460ms         2.74%        2.258s       3.216ms      29.064ms         0.04%        2.255s       3.213ms           702  \n",
      "                       aten::narrow         0.04%      33.959ms         0.07%      53.609ms      18.391us      38.840ms         0.05%      38.840ms      13.324us          2915  \n",
      "                          aten::log         0.04%      32.715ms         0.04%      32.715ms       2.726ms      32.555ms         0.04%      32.555ms       2.713ms            12  \n",
      "                    InverseBackward         0.04%      31.277ms         0.50%     411.760ms      68.627ms      30.151ms         0.04%     411.729ms      68.621ms             6  \n",
      "                  LinalgDetBackward         0.04%      30.591ms         3.19%        2.628s     437.926ms      29.604ms         0.04%        2.627s     437.897ms             6  \n",
      "                       aten::einsum         0.03%      27.945ms         1.57%        1.289s      33.913ms      24.803ms         0.03%        1.289s      33.930ms            38  \n",
      "                        aten::empty         0.03%      24.151ms         0.03%      24.151ms      12.520us       0.000us         0.00%       0.000us       0.000us          1929  \n",
      "                   aten::as_strided         0.03%      23.267ms         0.03%      23.267ms       3.243us       0.000us         0.00%       0.000us       0.000us          7175  \n",
      "                      aten::resize_         0.03%      21.676ms         0.03%      21.676ms      22.817us       0.000us         0.00%       0.000us       0.000us           950  \n",
      "                           aten::ge         0.03%      20.918ms         0.05%      43.789ms       1.216ms      21.728ms         0.03%      43.765ms       1.216ms            36  \n",
      "                           aten::to         0.03%      20.618ms         0.45%     366.557ms     411.861us      17.352ms         0.02%     360.374ms     404.915us           890  \n",
      "                       aten::select         0.02%      20.182ms         0.03%      24.807ms      18.134us      17.820ms         0.02%      17.820ms      13.027us          1368  \n",
      "                        aten::slice         0.02%      19.806ms         0.04%      31.638ms       7.220us       0.000us         0.00%       0.000us       0.000us          4382  \n",
      "                          aten::cat         0.02%      15.010ms         1.78%        1.469s       1.890ms       9.900ms         0.01%        1.485s       1.912ms           777  \n",
      "                           aten::lt         0.02%      14.813ms         0.04%      29.918ms       2.493ms      16.664ms         0.02%      33.699ms       2.808ms            12  \n",
      "                         aten::mul_         0.02%      13.301ms         0.10%      78.702ms       1.874ms      11.212ms         0.01%      77.667ms       1.849ms            42  \n",
      "                           aten::gt         0.02%      13.270ms         0.03%      28.102ms     669.103us      17.275ms         0.02%      35.219ms     838.543us            42  \n",
      "                        aten::zero_         0.01%      11.912ms         0.28%     230.539ms     576.348us       7.568ms         0.01%     239.820ms     599.549us           400  \n",
      "                          aten::all         0.01%       9.290ms         0.01%      10.232ms     568.471us       8.759ms         0.01%       8.896ms     494.231us            18  \n",
      "                         aten::rsub         0.01%       8.798ms         0.23%     185.262ms     964.904us       5.551ms         0.01%     184.075ms     958.726us           192  \n",
      "                aten::empty_strided         0.01%       8.484ms         0.01%       8.484ms      11.574us       0.000us         0.00%       0.000us       0.000us           733  \n",
      "                      aten::permute         0.01%       8.456ms         0.01%      10.470ms      24.929us       4.171ms         0.01%       4.171ms       9.931us           420  \n",
      "                     SymeigBackward         0.01%       8.273ms         0.17%     137.712ms      45.904ms       7.431ms         0.01%     138.213ms      46.071ms             3  \n",
      "             aten::_index_put_impl_         0.01%       7.440ms         0.06%      49.780ms       2.074ms       4.013ms         0.00%      46.451ms       1.935ms            24  \n",
      "                     aten::_s_where         0.01%       7.336ms         0.01%       7.355ms       2.452ms       7.680ms         0.01%       7.680ms       2.560ms             3  \n",
      "                          aten::cos         0.01%       6.530ms         0.01%       6.530ms       1.088ms       6.701ms         0.01%       6.701ms       1.117ms             6  \n",
      "                        aten::zeros         0.01%       6.348ms         0.29%     234.716ms     715.596us       7.605ms         0.01%     241.631ms     736.681us           328  \n",
      "               aten::slice_backward         0.01%       6.277ms         0.23%     191.822ms       1.015ms       7.259ms         0.01%     194.742ms       1.030ms           189  \n",
      "                      aten::reshape         0.01%       6.007ms         0.05%      44.962ms      69.926us       4.836ms         0.01%      41.419ms      64.415us           643  \n",
      "                       MulBackward0         0.01%       5.432ms         0.60%     496.377ms       5.705ms     507.839us         0.00%     496.054ms       5.702ms            87  \n",
      "                       PowBackward0         0.01%       5.250ms         0.12%      96.302ms       8.025ms       3.990ms         0.00%      97.204ms       8.100ms            12  \n",
      "        torch::autograd::CopySlices         0.01%       5.226ms         0.10%      78.741ms       1.250ms       2.808ms         0.00%      67.713ms       1.075ms            63  \n",
      "                         aten::view         0.01%       4.751ms         0.01%       4.751ms       5.745us       0.000us         0.00%       0.000us       0.000us           827  \n",
      "                        aten::clone         0.01%       4.358ms         1.71%        1.407s      10.989ms       1.707ms         0.00%        1.406s      10.984ms           128  \n",
      "                      aten::minimum         0.00%       4.099ms         0.01%       4.382ms     730.328us       5.392ms         0.01%       5.512ms     918.667us             6  \n",
      "            aten::new_empty_strided         0.00%       4.021ms         0.01%       4.999ms      79.355us       3.544ms         0.00%       3.544ms      56.257us            63  \n",
      "                          aten::dot         0.00%       3.682ms         0.00%       3.851ms     641.883us       1.948ms         0.00%       2.006ms     334.389us             6  \n",
      "                          aten::sin         0.00%       3.642ms         0.00%       3.642ms       1.214ms       3.395ms         0.00%       3.395ms       1.132ms             3  \n",
      "                      SliceBackward         0.00%       2.921ms         0.24%     194.743ms       1.030ms       1.515ms         0.00%     196.257ms       1.038ms           189  \n",
      "                       aten::arange         0.00%       2.522ms         0.01%       4.663ms      35.328us       2.277ms         0.00%       3.582ms      27.133us           132  \n",
      "              aten::select_backward         0.00%       2.521ms         0.10%      82.603ms       1.147ms       2.948ms         0.00%      92.192ms       1.280ms            72  \n",
      "                        aten::stack         0.00%       2.500ms         0.59%     486.225ms       6.483ms       2.320ms         0.00%     492.429ms       6.566ms            75  \n",
      "    torch::autograd::AccumulateGrad         0.00%       2.336ms         0.01%       6.945ms       2.315ms       2.128ms         0.00%       6.749ms       2.250ms             3  \n",
      "                           aten::eq         0.00%       2.157ms         0.01%       4.592ms     765.349us       2.248ms         0.00%       4.684ms     780.592us             6  \n",
      "                    aten::unsqueeze         0.00%       2.104ms         0.00%       3.403ms       8.007us       0.000us         0.00%       0.000us       0.000us           425  \n",
      "                       aten::matmul         0.00%       1.968ms         0.84%     688.760ms      28.698ms     205.146us         0.00%     688.914ms      28.705ms            24  \n",
      "                      RsubBackward1         0.00%       1.856ms         0.07%      53.749ms       1.493ms     769.344us         0.00%      50.786ms       1.411ms            36  \n",
      "     torch::autograd::CopyBackwards         0.00%       1.531ms         0.01%       7.421ms     117.787us       1.053ms         0.00%       4.432ms      70.348us            63  \n",
      "                       SubBackward0         0.00%       1.514ms         0.07%      58.126ms       1.292ms     410.976us         0.00%      57.570ms       1.279ms            45  \n",
      "                   aten::transpose_         0.00%       1.206ms         0.00%       2.022ms      36.112us     248.288us         0.00%     372.288us       6.648us            56  \n",
      "                       DivBackward0         0.00%       1.196ms         0.04%      34.023ms       1.031ms     281.664us         0.00%      38.729ms       1.174ms            33  \n",
      "                  aten::as_strided_         0.00%       1.154ms         0.00%       1.154ms      13.418us     220.384us         0.00%     220.384us       2.563us            86  \n",
      "                     SelectBackward         0.00%       1.120ms         0.10%      83.722ms       1.163ms       1.086ms         0.00%      93.278ms       1.296ms            72  \n",
      "                         aten::item         0.00%       1.065ms         0.00%       1.540ms      22.325us     374.208us         0.00%     546.208us       7.916us            69  \n",
      "                       BmmBackward0         0.00%       1.024ms         0.78%     645.035ms      43.002ms     107.178us         0.00%     647.528ms      43.169ms            15  \n",
      "                       RollBackward         0.00%       1.017ms         0.27%     225.964ms       4.185ms     214.435us         0.00%     224.821ms       4.163ms            54  \n",
      "                    aten::transpose         0.00%     999.106us         0.00%       1.613ms      13.909us       0.000us         0.00%       0.000us       0.000us           116  \n",
      "                   aten::empty_like         0.00%     985.465us         0.00%       4.066ms      21.978us       0.000us         0.00%       0.000us       0.000us           185  \n",
      "                 aten::masked_fill_         0.00%     978.101us         0.00%     978.101us     326.034us     584.544us         0.00%     584.544us     194.848us             3  \n",
      "                   aten::zeros_like         0.00%     954.966us         0.01%       5.460ms     101.119us       1.342ms         0.00%       2.550ms      47.218us            54  \n",
      "                      aten::inverse         0.00%     950.287us         9.56%        7.866s     393.311ms     128.844us         0.00%        7.867s     393.334ms            20  \n",
      "                    PermuteBackward         0.00%     854.598us         0.00%       1.883ms      31.377us     289.088us         0.00%     503.296us       8.388us            60  \n",
      "                       ViewBackward         0.00%     833.827us         0.05%      38.403ms     711.162us     255.134us         0.00%      37.025ms     685.647us            54  \n",
      "                       aten::expand         0.00%     771.340us         0.00%       1.149ms      10.544us       0.000us         0.00%       0.000us       0.000us           109  \n",
      "                          aten::eye         0.00%     688.789us         0.00%       1.503ms      50.112us     660.352us         0.00%       1.149ms      38.301us            30  \n",
      "                       aten::unbind         0.00%     621.383us         0.00%       1.331ms      73.964us     145.056us         0.00%     261.376us      14.521us            18  \n",
      "                  aten::result_type         0.00%     476.575us         0.00%     476.575us       9.929us     722.144us         0.00%     722.144us      15.045us            48  \n",
      "          aten::_local_scalar_dense         0.00%     475.877us         0.00%     475.877us       6.897us     172.000us         0.00%     172.000us       2.493us            69  \n",
      "                          aten::det         0.00%     467.809us         5.83%        4.800s     266.643ms     690.266us         0.00%        4.800s     266.682ms            18  \n",
      "                   aten::index_put_         0.00%     460.005us         0.06%      50.240ms       2.093ms     489.216us         0.00%      46.940ms       1.956ms            24  \n",
      "                       aten::symeig         0.00%     425.166us        60.28%       49.598s        8.266s      19.500us         0.00%       49.598s        8.266s             6  \n",
      "                     aten::diagonal         0.00%     411.591us         0.00%     567.353us      27.017us      37.632us         0.00%      37.632us       1.792us            21  \n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 82.279s\n",
      "Self CUDA time total: 82.384s\n",
      "\n",
      "CPU times: user 3min 54s, sys: 28.9 s, total: 4min 23s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#use_idty=True\n",
    "use_idty=False\n",
    "dim, sigma, epsilon, iter_num = 3., 0, 5e-3, 3\n",
    "height, width, depth = 145,174,145\n",
    "if not use_brain:\n",
    "  height, width, depth = 20,30,40\n",
    "  a = torch.rand((20,30,40, 3, 3))\n",
    "  a = 0.5 * (a + a.transpose(3, 4))\n",
    "  b = torch.rand((20,30,40, 3, 3))\n",
    "  b = 0.5 * (b + b.transpose(3, 4))\n",
    "\n",
    "#with torch.autograd.detect_anomaly():\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "  if use_brain:    \n",
    "    tens_0_def, phi_0, phi_inv_0 = metric_matching(torch.inverse(torch_tens_0), \n",
    "                                                   torch.inverse(torch_tens_1), \n",
    "                                                   height, width, depth, mask_union, iter_num, epsilon, sigma,dim,use_idty)\n",
    "  else:\n",
    "    a_def, phi_a, phi_inv_a = metric_matching(a, b, \n",
    "                                              height, width, depth, torch.ones((height,width,depth)), iter_num, epsilon, sigma,dim,use_idty)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "# Torch CPU 3 iters: CPU time: 7 min 17 s, Wall time: 2 min 7s\n",
    "# Torch CPU 3 iters: CPU time: 4min 7 s, Wall time: 1 min 39s\n",
    "# Torch GPU w/ anomaly detection 3 iters: CPU time: 40 min 20s, Wall time: 38 min 7s\n",
    "# Torch GPU 3 iters: CPU time: 39min 38s, Wall time: 37min 55s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del tens_0_def_GPU, phi_0_GPU, phi_inv_0_GPU\n",
    "#del tens_0_def, phi_0, phi_inv_0\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#a = torch.rand(5000, 3, 3,dtype=torch.double)\n",
    "#a = 0.5 * (a + a.transpose(1, 2))\n",
    "a = torch.rand(145,174,145, 3, 3,dtype=torch.double)\n",
    "a = 0.5 * (a + a.transpose(3, 4))\n",
    "w, wv = torch.symeig(a,True)  # fast (~0.0006s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.86 ms, sys: 0 ns, total: 9.86 ms\n",
      "Wall time: 6.84 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t, tv = vlinalg.vSymEig(a.permute((1,2,0)).reshape((1,9,-1,1,1)),True)\n",
    "e = t.reshape((3,-1)).permute((1,0))\n",
    "ev = tv.reshape((3,3,-1)).permute((2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0677, dtype=torch.float64) tensor(-1.0677)\n",
      "tensor(5.3951e-13, dtype=torch.float64) tensor(1.1847e-07, dtype=torch.float64)\n",
      "tensor(1.7764e-15, dtype=torch.float64)\n",
      "tensor(1.7419e-07, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "wediff = torch.abs(w-e)\n",
    "print(torch.min(w), torch.min(e))\n",
    "print(torch.min(wediff), torch.max(wediff))\n",
    "awv = torch.einsum('...ij,...jk->...ik',a,wv)\n",
    "wwv = torch.einsum('...i,...ji->...ji',w,wv)\n",
    "print(torch.max(torch.abs(awv-wwv)))\n",
    "aev = torch.einsum('...ij,...jk->...ik',a,ev.double())\n",
    "eev = torch.einsum('...i,...ji->...ji',e,ev)\n",
    "print(torch.max(torch.abs(aev-eev)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 93.8 ms, total: 1.78 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3, s3v = Sym3Eig.apply(a.reshape((-1,3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0677, dtype=torch.float64) tensor(-1.0677, dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64) tensor(2.9365e-14, dtype=torch.float64)\n",
      "tensor(5.3924e-13, dtype=torch.float64) tensor(2.9153, dtype=torch.float64)\n",
      "tensor(4.4846e-14, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "ws3diff = torch.abs(torch.sort(w)[0]-torch.sort(s3)[0])\n",
    "print(torch.min(w), torch.min(s3))\n",
    "print(torch.min(ws3diff), torch.max(ws3diff))\n",
    "es3diff = torch.abs(e-s3)\n",
    "print(torch.min(es3diff), torch.max(es3diff))\n",
    "as3v = torch.einsum('...ij,...jk->...ik',a,s3v.double())\n",
    "s3s3v = torch.einsum('...i,...ji->...ji',s3,s3v)\n",
    "print(torch.max(torch.abs(as3v-s3s3v)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 7.75 s, total: 8.81 s\n",
      "Wall time: 8.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = a.cuda()\n",
    "w, _ = torch.symeig(a)  # slow (~0.9s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.7 ms, sys: 106 µs, total: 4.81 ms\n",
      "Wall time: 3.59 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = vlinalg.vSymEig(a.permute((1,2,0)).reshape(1,9,-1,1,1))[0].reshape((3,-1)).permute((1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4409e-14, device='cuda:0', dtype=torch.float64) tensor(1.1909e-07, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "wediff = torch.abs(w-e)\n",
    "print(torch.min(wediff), torch.max(wediff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.33 ms, sys: 288 µs, total: 2.61 ms\n",
      "Wall time: 1.75 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3, _ = Sym3Eig.apply(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', dtype=torch.float64) tensor(2.9406, device='cuda:0', dtype=torch.float64)\n",
      "tensor(4.5131e-14, device='cuda:0', dtype=torch.float64) tensor(2.9406, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "ws3diff = torch.abs(w-s3)\n",
    "print(torch.min(ws3diff), torch.max(ws3diff))\n",
    "es3diff = torch.abs(e-s3)\n",
    "print(torch.min(es3diff), torch.max(es3diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsfcrcns",
   "language": "python",
   "name": "nsfcrcns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
